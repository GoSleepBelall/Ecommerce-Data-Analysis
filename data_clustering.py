# -*- coding: utf-8 -*-
"""Data Clustering.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1EDLQ7Nm8jed1nt0UG63OqOrIDaxais-p
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
from sklearn.cluster import DBSCAN



plt.style.use('seaborn-v0_8-pastel')
sns.set_palette("pastel")

# Get the current color palette
current_palette = plt.rcParams['axes.prop_cycle'].by_key()['color']

# Plot the colors in the palette
sns.palplot(current_palette)
plt.title("Current Color Palette")
plt.show()

"""# Read Data"""

data = pd.read_csv('/content/transformed_datatset_22_variables.csv')
data.head()

"""# Delete Non Contextual Variables

when applying clustering, we typically drop non-numeric columns such as customer ID, Transaction ID, Product ID and address. These columns are not suitable for clustering because clustering algorithm relies on numerical distances between data points. Categorical variables, like customer ID and address, don't have meaningful distances in a numerical sense.
"""

non_numeric_columns = ['Customer_ID', 'Address', 'Transaction_ID', 'Product_ID','Purchase_Date_Month_Year', 'Purchase_Date' ]
df_numeric = data.drop(non_numeric_columns, axis=1)

df_numeric.head()

"""# Perform One Hot Encoding

Now all we have are either numerical column or categorical column, Now it's time to convert the categorical columns to Numerical columns using One hot encoding.
"""

categorical_columns = ['Gender', 'Income_Level', 'Product_Category', 'Brand', 'Product_Category_Preferences', 'Season' , 'Age Group' , 'Product Range']
df_encoded = pd.get_dummies(df_numeric, columns=categorical_columns)

df_encoded.head()

"""# Scaling the features
Clustering Algorithms are sensitive to the scale of variables, so it's important to make all of them in a same scale.
"""

scaler = StandardScaler()
scaled_data = scaler.fit_transform(df_encoded)

#Convert to dataframe just to print head
df_scaled = pd.DataFrame(scaled_data)
df_scaled.head()

"""# K-Means Clustering

## Find Elbow

The inertia continues to decrease at a relatively consistent rate as the number of clusters increases. Although there isn't a clear "elbow" or point of inflection where the rate of decrease sharply changes, which typically indicates the optimal number of clusters.

In such cases where the elbow is not clearly defined, it's harder to pinpoint the exact optimal K. However, there does seem to be a slight bend in the curve around K = 4, where the rate of decrease in inertia begins to slow down. While not as pronounced as one might hope for in an elbow plot, this could suggest that K = 4 is a reasonable choice for the number of cluster.
"""

inertias = []
for k in range(2, 8):
    kmeans = KMeans(n_clusters=k, random_state=42)
    kmeans.fit(scaled_data)
    inertias.append(kmeans.inertia_)

# Plot the Elbow Method
plt.plot(range(2, 8), inertias, marker='o')
plt.xlabel('Number of Clusters (K)')
plt.ylabel('Inertia')
plt.title('Elbow Method for Optimal K')
plt.show()

"""## Fit the Model of K-means"""

optimal_k = 4
kmeans = KMeans(n_clusters=optimal_k, random_state=42)
df_encoded['cluster'] = kmeans.fit_predict(scaled_data)

"""## Use PCA to Visualize
PCA is a famous dimension reduction technique, as our dataset have 40 columns right now, It's impossible to visualize them without reducing the dimensions, Thus, We are reducting the dimensions to 2 in order to visualize.

"""

pca = PCA(n_components=2)
reduced_data = pca.fit_transform(scaled_data)

# Add the reduced dimensions to your dataframe
df_encoded['pca_one'] = reduced_data[:, 0]
df_encoded['pca_two'] = reduced_data[:, 1]

# Obtain the centroids from the kmeans object and transform them to PCA space
centroids_pca = pca.transform(kmeans.cluster_centers_)

# Now create a scatter plot with the two principal components
plt.figure(figsize=(10, 8))
sns.scatterplot(x="pca_one", y="pca_two", hue='cluster', data=df_encoded, palette=current_palette, legend="full")

# Plot the centroids in PCA space
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=100, c='black', marker='X', label='Centroids')
plt.title('K-Means Clustering Results with K=4')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()

plt.show()

"""# K-means++ Clustering
The random state is set to 20. When you set random_state to an integer, it ensures that the random numbers are generated in the same order each time the code is run. This means that every time you run the algorithm with the same random_state, you will get the same results.
"""

kmeans = KMeans(n_clusters=optimal_k, init='k-means++', random_state=20)
df_encoded['cluster++'] = kmeans.fit_predict(scaled_data)

"""## PCA Dimension Reduction to visualize"""

pca = PCA(n_components=2)
reduced_data = pca.fit_transform(scaled_data)

df_encoded['pca_one'] = reduced_data[:, 0]
df_encoded['pca_two'] = reduced_data[:, 1]

centroids_pca = pca.transform(kmeans.cluster_centers_)

#Plotting
plt.figure(figsize=(10, 8))
sns.scatterplot(x='pca_one', y='pca_two', hue='cluster', data=df_encoded, palette=current_palette, legend="full")
plt.scatter(centroids_pca[:, 0], centroids_pca[:, 1], s=100, c='black', marker='X', label='Centroids')

plt.title('K-Means Clustering Results with K={}'.format(optimal_k))
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.show()

"""# Difference between both algorithms & why they both show almost same results?
K-means and K-means++ are both iterative algorithms used for clustering. The primary difference between them lies in the initialization of the centroids:

- K-means randomly selects initial centroids from the data points, which could lead to poor performance or slow convergence if the initial centroids are not well chosen.

- K-means++ is designed to overcome this potential pitfall by using a smarter initialization technique that tends to spread out the initial centroids. This is achieved by selecting the first centroid randomly, and then the subsequent centroids are chosen from the remaining data points with probability proportional to the square of the distance from the nearest existing centroid. This generally leads to a better starting point for the algorithm.

Despite the differences in initialization, both algorithms proceed with the same iterative process after the initial centroids are chosen. They alternate between assigning points to the nearest centroid and recalculating the centroids based on the assignments.

The reason K-means and K-means++ often end up at similar centroids is due to the nature of the optimization problem they are solving. They both aim to minimize the same objective function, which is the sum of squared distances between data points and their nearest centroid. If the data has clear, well-separated clusters, both methods are likely to converge to similar solutions, regardless of the initial centroids, because there may be a strong natural clustering structure in the data that dominates the results.

However, K-means++ tends to provide better and more consistent results because its initialization method reduces the likelihood of falling into local minima, which can be a problem with plain K-means, especially when the initial random centroids are poorly chosen.

# DBSCAN

After Manually trying multiple values for **eps** and **min_samples**, We found this combination with minimum noise.
"""

dbscan = DBSCAN(eps=10, min_samples=2)
df_encoded['dbscan_cluster'] = dbscan.fit_predict(scaled_data)

"""## PCA dimension Reduction to visualize"""

pca = PCA(n_components=2)
reduced_data = pca.fit_transform(scaled_data)
df_encoded['pca_one'] = reduced_data[:, 0]
df_encoded['pca_two'] = reduced_data[:, 1]

# Plotting
plt.figure(figsize=(10, 8))
sns.scatterplot(x='pca_one', y='pca_two', hue='dbscan_cluster', data=df_encoded, palette=current_palette, legend="full")
plt.title('DBSCAN Clustering Results')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.legend()
plt.show()