# -*- coding: utf-8 -*-
"""Data Reading & Cleaning.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1GTo5hFs-qrYaVsFjBU9yxcKjmb9mW8R4
"""

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import numpy as np
import json


plt.style.use('seaborn-v0_8-pastel')
sns.set_palette("pastel")

# Get the current color palette
current_palette = plt.rcParams['axes.prop_cycle'].by_key()['color']

# Plot the colors in the palette
sns.palplot(current_palette)
plt.title("Current Color Palette")
plt.show()

"""# Data Reading
It is observed that the dataset is not in the default format that can be read by using default functions, thus we will read the dataset line by line and parse it accordingly. Once the json is parsed, We will convert it to Pandas dataframe.
"""

# Reading the JSON file
path = '/content/electronics.json'
with open(path, 'r') as file:
    data_json = json.load(file)

# Converting the JSON object to a pandas DataFrame
data = pd.json_normalize(data_json)
data.head()

"""# Analyzing Datatypes

Before doing any analyis, it is important to analyze the datatypes of dataset. Due to a different methodology of reading the dataset, We can clearly see that every feature is shown as "object" right now. We need to change these columns accordingly.
"""

# Get the data types of all columns
print(data.dtypes)

"""## Conversion of Datatypes
The manual verification of dataset shows that there are some categorical (qualitative) features however there are some numerical (quantitative) features. We will convert them accordingly.
"""

# Convert numerical columns to numeric type
numerical_columns = ['Age', 'Purchase_Amount', 'Average_Spending_Per_Purchase',
                     'Purchase_Frequency_Per_Month', 'Brand_Affinity_Score']
data[numerical_columns] = data[numerical_columns].apply(pd.to_numeric, errors='coerce')

# Convert 'Purchase_Date' to datetime
data['Purchase_Date'] = pd.to_datetime(data['Purchase_Date'], errors='coerce')

# Convert categorical columns to 'category' data type
categorical_columns = ['Gender', 'Income_Level', 'Product_Category', 'Brand',
                       'Product_Category_Preferences', 'Season']
data[categorical_columns] = data[categorical_columns].astype('category')

data['Month'] = pd.to_numeric(data['Month'], errors='coerce').astype('Int64')
data['Year'] = pd.to_numeric(data['Year'], errors='coerce').astype('Int64')

print(data.dtypes)

data.head()

"""# Descriptive Analysis
A brief overview summary of descriptive analysis for the dataset is very important to understand the nature of dataset, specially to analyze the missing values.
"""

np.shape(data)

descriptive_stats = data.describe()
descriptive_stats

# checking for missing values after conversion
new_missing_values = data.isnull().sum()
new_missing_values

"""# Data Cleaning

## Missing values
It is really important to analyze all features and check accordingly. If any column has a very high percentage of missing values (typically, more than 50-60%), it might be worth considering dropping the column. However, in this dataset, it seems that the maximum missingness in any column is not exceedingly high. Now the need is to check all features one by one, and check if we can use mean or median for imputation. The choice between mean and median depends on the distribution of feature. If it's normally distributed, mean is suitable; if skewed, median is preferable as it is more robust to outliers and skewed distributions. It represents the middle value.

Now we will analyze every feature one by one:

### Age:
The histogram of Age suggests that the distribution is relatively symmetric and does not show extreme skewness. Therefore, we can use the mean for imputation.
"""

plt.figure(figsize=(10, 6))
data['Age'].hist(bins=30)
plt.title('Histogram of Age')
plt.xlabel('Age')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

"""
After this mean imputation there will be 0 null values in Age feature."""

age_mean = data['Age'].mean()
print("imputing with: ", age_mean)
data['Age'].fillna(age_mean, inplace=True)

"""### Purchase Ammount:
The histogram of Purchase ammount appears somewhat skewed. Median imputation might be more appropriate here.
"""

plt.figure(figsize=(10, 6))
data['Purchase_Amount'].hist(bins=30)
plt.title('Histogram of Purchase Amount')
plt.xlabel('Purchase Amount')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

purchase_amount_median = data['Purchase_Amount'].median()
print("imputing with: ", purchase_amount_median)
data['Purchase_Amount'].fillna(purchase_amount_median, inplace=True)

"""### Average Spending Per Purchase:
This distribution also seems slightly skewed. Median imputation would be suitable.
"""

plt.figure(figsize=(10, 6))
data['Average_Spending_Per_Purchase'].hist(edgecolor='black')
plt.title('Histogram of Average Spending Per Purchase')
plt.xlabel('Average Spending Per Purchase')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

average_spending_median = data['Average_Spending_Per_Purchase'].median()
print("imputing with: ", average_spending_median)
data['Average_Spending_Per_Purchase'].fillna(average_spending_median, inplace=True)

"""### Purchase Frequency Per Month
The distribution is relatively symmetric, making mean imputation a good choice.
"""

plt.figure(figsize=(10, 6))
data['Purchase_Frequency_Per_Month'].hist(edgecolor='black')
plt.title('Histogram of Purchase Frequency Per Month')
plt.xlabel('Purchase Frequency Per Month')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

purchase_frequency_mean = data['Purchase_Frequency_Per_Month'].mean()
print("imputing with: ", purchase_frequency_mean)
data['Purchase_Frequency_Per_Month'].fillna(purchase_frequency_mean, inplace=True)

"""### Brand Affinity Score:
This distribution is fairly uniform. We will use mean imputation here
"""

plt.figure(figsize=(10, 6))
data['Brand_Affinity_Score'].hist(edgecolor='black')
plt.title('Histogram of Brand Affinity Score')
plt.xlabel('Brand Affinity Score')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

brand_affinity_mean = data['Brand_Affinity_Score'].mean()
print("imputing with: ", brand_affinity_mean)
data['Brand_Affinity_Score'].fillna(brand_affinity_mean, inplace=True)

"""### Year:
This is a significant time marker, using the median (a more robust measure) could be a reasonable approach, using the median is often a safer choice to avoid introducing bias, especially if the dataset spans several years where the mean could be affected by historical data trends. Moreover, another point is that mean can provide non-numeric values (in decimal) which doesn't make sense in case of Year.
"""

plt.figure(figsize=(10, 6))
data['Year'].hist(edgecolor='black')
plt.title('Histogram of Year')
plt.xlabel('Year')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

year_median = data['Year'].median()
print("imputing with: ", year_median)
data['Year'].fillna(year_median, inplace=True)

"""### Month:
Similar methodology that are implemented for Year would be used for Month as it is also a time marker.
"""

plt.figure(figsize=(10, 6))
data['Month'].hist(edgecolor='black')
plt.title('Histogram of Month')
plt.xlabel('Month')
plt.ylabel('Frequency')
plt.grid(False)
plt.show()

month_median = data['Month'].median()
print("imputing with: ", month_median)
data['Month'].fillna(month_median, inplace=True)

"""### Purchase Date:
Given that the dataset contains 1000 rows and only 48 are missing Purchase_Date, this represents 4.8% of dataset. Since the missing Purchase_Date data is less than 5% of the total dataset, it is reasonable to simply drop these rows. This is often the simplest and most straightforward approach, especially if the missing dates are randomly distributed and their removal won't introduce significant bias.
"""

data.dropna(subset=['Purchase_Date'], inplace=True)

"""## Outlier Detection
In data cleaning, it's important to check outliers and their influence on datasets. Outliers are only associated with numerical variables. So, to detect outliers We will plot box-plot of all numerical variables.
"""

numerical_columns = [
    'Age', 'Purchase_Amount', 'Average_Spending_Per_Purchase',
    'Purchase_Frequency_Per_Month', 'Brand_Affinity_Score', 'Month', 'Year'
]

# Creating box plots for each numerical column
plt.figure(figsize=(15, 10))

for i, column in enumerate(numerical_columns, 1):
    plt.subplot(3, 3, i)
    sns.boxplot(y=data[column])
    plt.title(column)

plt.tight_layout()
plt.show()

"""Although the Box plots clearly shows their are no outliers in any numerical column, still to verify, We will further implement Interquartile Range (IQR) method to verify that there is no outlier. The IQR method typically considers values as outliers if they are below the 1st quartile - 1.5 * IQR or above the 3rd quartile + 1.5 * IQR.
By the outcome of the following code snippet it is further established that there is no outlier that is need to reduced/normalized or removed.
"""

# Calculate IQR for each numerical column
Q1 = data[numerical_columns].quantile(0.25)
Q3 = data[numerical_columns].quantile(0.75)
IQR = Q3 - Q1

# Determine the outliers for each column
outliers = ((data[numerical_columns] < (Q1 - 1.5 * IQR)) | (data[numerical_columns] > (Q3 + 1.5 * IQR)))

# Count of outliers in each column
outlier_counts = outliers.sum()
print(outlier_counts)

"""## Descriptive Analysis After Cleaning
As there are multiple changes in features and some of the rows are dropped, The descriptive Analysis is completly changed, with different stats altogeather.
"""

np.shape(data)

descriptive_stats = data.describe()
descriptive_stats

# Rechecking for missing values after conversion
new_missing_values = data.isnull().sum()
new_missing_values

"""## Data Saving
Now save this cleaned dataset for further exploration
"""

data.to_csv('cleaned_datatset_952_rows.csv', index=False)